---
title: "A cursory view on Generalised Additive (Mixed) Models (GAMMs)"
author: "Michele Gubian"
date: "March 2022"
output: html_document
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggthemes)
library(magrittr)
library(mgcv)
library(itsadug)
library(emmeans)
library(mvtnorm)
library(clusterGeneration)
select <- dplyr::select # conflict with MASS
```

# A naive approach to modelling curves

Suppose we have several realisations of a pitch accent in two conditions.
For simplicity:

* no random factors, e.g. all utterances from the same speaker
* all utterances examined on the same time interval, 0-2 sec
* or we might imagine that limear time normalisation was applied


```{r echo=FALSE, message=FALSE}
home_dir <- "/vdata/ERC2/FPCA/FPCA-phonetics-workshop"
# home_dir <- "C:/Users/Michele/Dropbox/scambio_temp/work/FDA/FPCA-phonetics-workshop"
data_dir <- file.path(home_dir,'data')
ex <- 1 
curves <- read_csv(file.path(data_dir, paste("ex1D", ex, "csv", sep = '.')))
nCurves <- curves %>% select(curveId) %>% n_distinct()
curves %<>% mutate(Category = factor(Category))

Category.colors <- c("slategray4", "orangered")
# plot a few curves
ggplot(curves %>% filter(curveId %in% sample(nCurves, 20))) +
  aes(x = time, y = y, group = curveId, color = Category) +
  geom_line() +
  scale_color_manual(values=Category.colors) +
  theme_light() +
  theme(text = element_text(size = 15))

```

We could try something like this and use `lm()`:

$$
y_i = \beta_0 + \beta_{0, P} + \beta_1 t_i + \beta_{1, P} t_i +   \beta_2 t_i^2 + \beta_{2, P} t_i^2 + 
\beta_3 t_i^3 + \beta_{3, P} t_i^3 + ... + \epsilon_i 
$$
where:

* $P$ indicates the PEAK category, while the default (control) level is NO_PEAK
* instead of $x$ we have $t$ for time
* $i$ is **not** an index for an entire curve, but for a single sample

In R it would look like this:

```{r echo=TRUE, eval=FALSE}
mod <- lm(y ~ Category + time + Category:time + I(time^2) + Category:I(time^2) + I(time^3) + Category:I(time^3) , data = curves)
```

although this is quite a naive approach, see orthogonal polynomials (`poly` in R).

### Problems

* We don't know where to stop with power terms
    + but we could use model selection 
* Pure power series are really bad at interpolating general shapes
    + because Taylor's theorem is about approximating curves near a specific point, not globally
* How are we going to use the $\beta$ terms for insight and interpretation?

# GAMMs

* Use **SPLINES** as base (polynomial) functions, because they are good for interpolating any shape
* Use **regularisation** and **Cross-Validation** to determine complexity (how many functions) and smoothness 
* **Interpret shapes**, not coefficients

[Slides by Simon Wood](https://www.maths.ed.ac.uk/~swood34/talks/Beijing15.pdf)

The model looks like:

$$
y_i = f(t_i) + f_P(t_i) + \epsilon_i 
$$
where each $f()$ is a SPLINE:
$$
f(t) = \sum_{k=1}^K \beta_k \cdot \phi_k(t)
$$
In the GAMMs lingo, the $f()$ are called **smooths**. 
The functions $\phi_k(t)$ are the spline components, they are known, they play the same role as the powers of $t$ in the previous equation.
In simple terms, we can imagina a smooth to be like a polynomial of unspecified degree, or in general a functional term of unspecified complexity, i.e. we do not say how many terms it should include. The GAMM machinary takes care of determining how many terms are strictly needed for our problem. 

## GAMMs in R

* Package `mgcv` is the most popular for GAMMs within R
* The main command is `bam()`, equivalent to `lmer()` in `lme4`
* The R `formula` syntax is quite different from the one used in `lme4`
* The model specification with `bam()` has more possibilities than `lmer()`
    + fixed and random predictors can be declaired as *parametric* (like with `lmer()`) or as *smooths*, or combinations
    + interactions between covariates (= continuous predictors) has different options based on *tensors*
* Interaction between factors (`a * b`) **is not available with smooths**
    + the user has to create their own `interaction` terms manually beforehand (more on this later)


## Specifying fixed effects 

### A single smooth

| Equation  | R formula |
|:---------:|:---------:|
| $y_i = \beta_0 + f(t_i) + \epsilon_i$ | `y ~ s(time)` |

Note that regular smooths like `s(time)` are **centered on zero**, hence the intercept term in the equation. The model `summary()` will show two separate terms: 

* `(Intercept)`, i.e. $\beta_0$
* `s(time)`, i.e. the zero-centered function $f(t_i)$

Example:

```{r echo=FALSE, message=FALSE}
# plot a few curves
ex <- 2
curves <- read_csv(file.path(data_dir, paste("ex1D", ex, "csv", sep = '.'))) %>%
  filter(Category == "NO_PEAK") %>%
  mutate(y = y + rnorm(n(), 0, 0.2))
ggplot(curves %>% filter(curveId %in% sample(nCurves, 20)) %>% mutate(curveId = factor(curveId)) ) +
  aes(x = time, y = y, group = curveId, color = curveId) +
  geom_line() +
  theme_light() +
  theme(text = element_text(size = 15), legend.position = "none")
```



```{r}
m1 <- bam(y ~ s(time), data = curves)
summary(m1)
```
### Reading the summary

* `Parametric coefficients`: the `lm`-style fixed effects
* `edf`: estimated degrees of freedom of smooth terms. More or less, how many coefficients you need to specify the shape of a smooth. The default max number of coefficients is `k = 10`, minus 1, as the smooth is zero-centered, so the intercept is already specified
* `fREML`: fast restricted maximum likelihood estimation. Cannot be used to compare models with different fixed effects composition. For this you need to re-run your models in `ML` modality, see section 4.5.1 in  [Wieling's tutorial](http://martijnwieling.nl/files/GAM-tutorial-Wieling.pdf).
* `Scale est.`: residual variance
* `n`: number of samples
    + number of curves? missing! (more on this later)

### Plotting

* Plotting is essential!
* Make sure you know what you are plotting (see [itsadug  docs](https://cran.r-project.org/web/packages/itsadug/vignettes/overview.html)):
    + partial effects
    + summed effects
    
**Partial effects** are smooths, which are zero centered in their default mode. This means that they represent the 'shape part' of the effect, i.e. the total effect minus the mean across the time axis (or whatever continuous variable you are using)

Smooths are (implicitly) numbered by their order of appearance in the `summary()` output. To plot a selected smooth, two options (among many):

First option: use `mgcv::plot`:
```{r}
plot(m1, select = 1, shade = TRUE, rug = FALSE)
abline(h=0, col='red', lty=2)
```

Second option: extract fitting data from model, then plot it with `ggplot` or `plotfunctions::plot_error`.
```{r}
smooth1 <- get_modelterm(m1, select = 1)
smooth1$time
smooth1$fit
smooth1$se.fit
ggplot(smooth1) +
  aes(x = time, y = fit) +
  geom_line() +
  geom_ribbon(mapping = aes(ymin = fit - se.fit, ymax = fit + se.fit), fill = "red", alpha = 0.5) +
  theme_light() +
  theme(text = element_text(size = 15))
```

**Summed effects** include smooths and intercept (parametric) terms that pertain to an effect. In our case it means $\beta_0 + f(t_i)$.

The easy way to plot summed effects is to use `itsadug::plot_smooth`:
```{r}
plot_smooth(m1, view = "time", rug = FALSE)
```

One could also get all the relevant values and sum them manually. In our case, get samples from the smooth $f(t_i)$ using `get_modelterm` as above, then add the value of $\beta_0$ taken form the summary (i.e. `m1$coefficients["(Intercept)"]`).

**Question:** the confidence intervals are quite thin in comparison with the impression of noisy signals from the raw data plot. Why is that?

### Diagnostics



#### Degrees of freedom

The specification of smooths `s()` has an extra argument `k` whose default value is 10. `k` indicates the maximum number of splines allowed to specify the smooth, or in other words the maximum number of parameters or degrees of freedom.
As regular smooths are zero-centered, this means that a smooth with at most `k = 10` parameters has actually at most `k' = 9` degrees of freedom, as one is absorbed by the intercept term. The estimated degrees of freedom `edf` is an indicator for how many 'equivalent parameters' were actually utilised in the estimated smooth. When `edf` is near 1, it means that it's basically a straight line (only the slope parameter), in which case we would rather remove the smooth and use an ordinary linear term. On the contrary, when `edf` is very close to `k' = k-1` as in this case, it means that it has used up all its degrees of freedom, thus perhaps more might be needed. 

Let's specify a higher `k` and see what happens to `edf` for `s(time)`:

```{r}
m1.k20 <- bam(y ~ s(time, k = 20), data = curves)
summary(m1.k20)
```
`edf` has not increased much, and it's well below `k - 1 = 19`, so the limit on `k = 10` was ok.

#### Residuals

```{r}
gam.check(m1)
```

* Quantile plot and histogram indicate how close to Gaussian the distribution of residuals is, i.e. how close we are to the hypothesis $\epsilon \sim N(0, \sigma)$.
* Residuals vs. linear predictor shows whether trends are visible in the residuals -- which they shouldn't. For example, if residuals have a wider span for some values of the predicted output it means that the data are heteroscedastic, i.e. the variance of the residuals is not independent of predicted output, but it varies with it. Sometimes, especially with time series, this problem is mitigated by using a different distribution for the residuals, the so-called *scaled-t*. But often it also means that the model is ill-specified, and a different predictor structure is needed.
* Response vs. For Gaussian residuals, Fitted Values provides a similar picture as Residuals vs. linear predictor. The difference is that the y axis reports the actual output values (`y` in the example), and not the residual errors.



### A fixed factor interacting with a smooth

```{r echo=FALSE, message=FALSE}
ex <- 1 
curves <- read_csv(file.path(data_dir, paste("ex1D", ex, "csv", sep = '.')))
nCurves <- curves %>% select(curveId) %>% n_distinct()
curves %<>% mutate(Category = factor(Category))

# plot a few curves
ggplot(curves %>% filter(curveId %in% sample(nCurves, 20))) +
  aes(x = time, y = y, group = curveId, color = Category) +
  geom_line() +
  theme_light() +
  scale_color_manual(values=Category.colors) +
  theme(text = element_text(size = 15))
```

| Equation  | R formula |
|:---------:|:---------:|
| $y_i = \beta_0 + \beta_P + f_{NP}(t_i) + f_P(t_i) + \epsilon_i$ | `y ~ Category + s(time, by = Category)` |

where $P$ indicates the PEAK category, while the default (control) level is NO_PEAK ($NP$), and $i$ is the sample (not curve) index.
The equation is omitting the indicator functions. The complete version is:
$$ y_i = \beta_0 + \beta_P \cdot I(i \in P) + f_{NP}(t_i) \cdot I(i \in NP) + f_P(t_i) \cdot I(i \in P) + \epsilon_i $$
Note the asymmetry in the conventions for the parametric and the smooth parts of the model:

* Parametric part: as in `lm()`, $\beta_0$ is the intercept, $\beta_P$ is the difference between level $P$ and the intercept
* Smooths: one for each level of the fixed factor, all zero-centered

**Question:** What are the summed effects for the PEAK and the NO_PEAK levels (in maths notation)?

Estimate the model with `bam()`:
```{r}
curves %<>% mutate(Category = factor(Category)) # don't forget!
m2 <- bam(y ~ Category + s(time, by = Category), data = curves)
summary(m2)
```
**Warning:** all variables that need to be used as fixed or random factors (here `Category`) **must** be converted to R class `factor`, otherwise `bam` will throw an error. 

**Question:** is PEAK significantly different from NO_PEAK?



We can read off the `summary` output the statistical significance of parametric terms in the same way as with `lm`. In geometrical terms, we can say whether the average level of `y` along the time axis is different for PEAK and NO_PEAK curves. 

With the model expressed as above, we **cannot** use the significance levels in the  `summary` to answer questions about the difference between smooths. A small p-value only says that the smooth is significantly different from zero, which means that its shape is different from the flat line `y = 0`. If both PEAK and NO_PEAK smooths appear with a small p-value, it means that their shapes are not flat, i.e. in both cases `y` varies non-linearly with `time`, but nothing can be said about whether their respective shapes are similar or not.

Let's plot first. A look at the summed effects:

```{r}
plot_smooth(m2, view = "time", plot_all = "Category", col = Category.colors)
```

A graphical way to detect statistical differences between effects is:

```{r}
plot_diff(m2, view = "time", comp = list(Category = c("PEAK", "NO_PEAK")))
```

### Testing for significance: `emmeans`

The powerful `emmeans` function can be used on `mgcv` GAMMs as a way to test pointwise comparisons. Suppose we want to test whether PEAK and NO_PEAK differ at `time` equal 0.0, 1.0 and 2.0. We can use `emmeans` in the same way as we do with `lm` or `lmer`, and we will get the correction for multiple comparisons.

```{r}
emmeans(m2, pairwise ~ Category | time, at = list(time = 0:2))
```

### Testing for significance: binary smooths

While `emmeans` allows to test differences at specific points on the time axis (or other continuous dimensions), we need a way to assess for the significance of a smooth contrast as a whole. For the purpose, we utilise another way to specify a smooth interacting with a fixed factor with `mgcv::bam`, the so-called *binary smooths*.

A binary smooth is a smooth that represents the difference between effects (including intercepts). We use them to model the  difference between two levels of a factor, or between more complex combinations of factor level interactions (more on this later). Once we are able to introduce such a term in the model, we obtain two things:

1. we can assess the statistical significance of the  difference between a pair of levels of interest by reading it off the model `summary`
2. we can plot such difference and interpret it

In order to define a convenient binary smooth set up with `mgcv`, 
first we need to define **numeric variables with the meaning of logical variables** (yes you read it right), which represent the desired contrasts -- a rather obscure convention adopted by the `mgcv` library. In our case we want to represent the difference between PEAK and  NO_PEAK, with NO_PEAK as control level:

```{r}
curves$IsPEAK <- (curves$Category == "PEAK") * 1
```

where  `* 1` is a quick way to transform the logical value inside `()` into a numeric one. The R formula and the corresponding maths form are:

| Equation  | R formula |
|:---------:|:---------:|
| $y_i = \beta_0 + f_0(t_i) + b_P(t_i) \cdot I(i \in P) + \epsilon_i$ | `y ~ s(time) + s(time, by = IsPEAK)` |

where:

* $\beta_0$ is the global intercept
* $f_0(t_i)$ is the global, zero-centered smooth
* $b_P(t_i)$ is the **not zero-centered** binary smooth. It represents the total difference between PEAK and  NO_PEAK

```{r}
m2.bin <- bam(y ~ s(time) + s(time, by = IsPEAK), data = curves)
summary(m2.bin)
```
| Maths  | R summary |
|:---------:|:---------:|
| $\beta_0$ | `(Intercept)` |
| $f_0(t_i)$  | `s(time)` |
| $b_P(t_i)$ | `s(time):IsPEAK` |

Only now we are able to report about the significance of the difference between PEAK and NO_PEAK by reading off the `summary` output for `s(time):IsPEAK`.

How does the difference look?
```{r}
get_modelterm(m2.bin, select = 2) %>%
  ggplot() +
  aes(x = time, y = fit) +
  geom_line() +
  geom_ribbon(mapping = aes(ymin = fit - se.fit, ymax = fit + se.fit), fill = "red", alpha = 0.5) +
  theme_light() +
  theme(text = element_text(size = 15))
```

There is a third modality for smooths, the so-called *ordered factor difference smooth*. It allows to separate the intercept difference form the shape difference, i.e. it splits the $b_P(t_i)$ term into a constant and a zero-centered smooth. See section 4.5.3 in [Wieling's tutorial](http://martijnwieling.nl/files/GAM-tutorial-Wieling.pdf) for details. 

## Two interacting covariates -- smooth surfaces

Suppose we want to model the effect of trial, i.e. the shape of the curve `y(time)` does change not only according to a category (PEAK, NO_PEAK) but also along a numeric variable `trial` encoding at which point in the experiment we are. The data look like this:

```{r echo=FALSE, message=FALSE}
ex <- 1 
curves <- read_csv(file.path(data_dir, paste("ex1D", ex, "csv", sep = '.')))
nCurves <- curves %>% select(curveId) %>% n_distinct()
h1_ <- -3e-3; m1_ <- 1; s1_ <- .5
curves %<>% 
  inner_join(curves %>% distinct(curveId) %>% mutate(trial = sample(n(), replace = TRUE))) %>%
  mutate(across(c(Category, curveId), ~ factor(.x))) %>%
  group_by(curveId) %>%
  mutate(y = y + trial * h1_ * exp(-((time-m1_)/s1_)**2)) %>%
  ungroup()

rm(h1_, m1_, s1_)

# plot a few curves
ggplot(curves %>% filter(curveId %in% sample(nCurves, 20))) +
  aes(x = time, y = y, group = curveId, color = trial) +
  geom_line() +
  facet_grid(~ Category) +
  theme_light() +
  theme(text = element_text(size = 15))
```

This is a case of a two-dimensional, nonlinear interaction between two covariates, `time` ($t$) and `trial` $(r)$. The default way is to use a *tensor*. 

| Equation  | R formula |
|:---------:|:---------:|
| $y_i = \beta_0 + \beta_P + f_{NP}(t_i, r_i) + f_P(t_i, r_i) + \epsilon_i$ | `y ~ Category + te(time, trial, by = Category)` |

Each of the tensor functions $f$ is the generalisation of a complete interaction expression in `lm`:

| Maths      | R formula in lm() |
|:---------:|:----------:|
| $\beta_1 \cdot t_i + \beta_2 \cdot r_i + \beta_3 \cdot t_i \cdot r_i$ | `time * trial`, or `time + trial + time:trial` |

```{r}
m3 <- bam(y ~ Category + te(time, trial, by = Category), data = curves)
summary(m3)
```
The surface plot for the PEAK and NO_PEAK conditions and slices along `trial == 20` and `trial == 80` (all summed effects) are:

```{r echo=FALSE, warning=FALSE, fig.show="hold", out.width="50%",message=FALSE}
fvisgam(m3, view = c("time", "trial"), cond = list(Category = "NO_PEAK"), color = c('white', 'blue'), main = "NO_PEAK", print.summary = FALSE)
abline(h=c(20, 80),lty=2,lwd=2,col="red")
fvisgam(m3, view = c("time", "trial"), cond = list(Category = "PEAK"), color = c('white', 'blue'), main = "PEAK", print.summary = FALSE)
abline(h=c(20, 80),lty=2,lwd=2,col="red")
plot_smooth(m3, view = "time", plot_all = "Category", cond = list(trial = 20), col = Category.colors, lwd=2, main = "trial = 20", print.summary = FALSE)
plot_smooth(m3, view = "time", plot_all = "Category", cond = list(trial = 80), col = Category.colors, lwd=2, main = "trial = 80", print.summary = FALSE)
fvisgam(m3, view = c("time", "trial"), cond = list(Category = "NO_PEAK"), plot.type = "persp", theta = 30, phi = 30, color = c('blue', 'white'), main = "NO_PEAK", print.summary = FALSE, zlab = "y")
fvisgam(m3, view = c("time", "trial"), cond = list(Category = "PEAK"), plot.type = "persp", theta = 30, phi = 30, color = c('blue', 'white'), main = "PEAK", print.summary = FALSE, zlab = "y")
```

If we were using `lm` instead of `bam` we would be able to produce similar surface visualisations, but in this case pretty bad estimates:

```{r}
m3.lm <- lm(y ~ Category * time * trial, data = curves)
 summary(m3.lm)
```
```{r echo=FALSE, warning=FALSE, fig.show="hold", message=FALSE}
# construct predictions on a regular grid
m3.lm.pred <- expand_grid(curves %>% distinct(Category, time), trial = 1:100) %>%
  arrange(Category, trial, time) %>%
  mutate(y = predict(m3.lm, .))
ggplot(m3.lm.pred) +
  aes(time, trial, fill = y) +
  # stat_contour_filled(bins = 9) + 
  geom_raster() +
  facet_grid(~ Category) +
  # scale_fill_brewer() +
  scale_fill_gradient(low = 'white', high = 'blue') +
  geom_hline(yintercept = c(20, 80), linetype = "dashed", color = "red") +
  theme_light() +
  theme(text = element_text(size = 15),
        legend.position = "bottom")

m3.lm.pred %>%
  filter(trial %in% c(20, 80)) %>%
  ggplot() +
  aes(time, y, color = Category) +
  geom_line() +
  scale_color_manual(values=Category.colors) +
  facet_grid(~ trial, labeller = label_both) +
  theme_light() +
  theme(text = element_text(size = 15), legend.position = "bottom")

```

```{r echo=FALSE, warning=FALSE, fig.show="hold", out.width="50%",message=FALSE}
for (categ in c("NO_PEAK", "PEAK")) {
persp(x = m3.lm.pred$time %>% unique() %>% sort(),
      y = m3.lm.pred$trial %>% unique() %>% sort(),
      z = m3.lm.pred %>% filter(Category == categ) %>% pivot_wider(names_from = "trial", values_from = "y") %>% select(-c(Category, time)) %>% as.matrix(),
      xlab = "time", ylab = "trial", zlab = "y", main = categ, 
      theta = 30, phi = 30, col = c('blue', 'white'), border = NA
) 
}
```

The surfaces themselves are non-linear, i.e. they are not flat, because of the product term $\beta_3 \cdot t_i \cdot r_i$, but each slice along either `time` or `trial` is a straight line (this is called a *ruled surface*). 

To test for significance differences between PEAK and NO_PEAK, we can use binary smooth surfaces as above:

```{r warning=FALSE}
curves$IsPEAK <- (curves$Category == "PEAK") * 1
m3.bin <- bam(y ~ te(time, trial) + te(time, trial, by = IsPEAK), data = curves)
summary(m3.bin)
pvisgam(m3.bin, view = c("time", "trial"), select = 2, color = c('white', 'blue'), main = "PEAK - NO_PEAK", print.summary = FALSE)
```

The difference PEAK - NO_PEAK is then significant, but the surface plot suggests that such difference does not change much along `trial`. To disentangle this, another form of tensor product is available, `ti`, which isolates the interaction:

| Equation  | R formula |
|:---------|:---------|
| $y_i = \beta_0 + \beta_P +$ | `y ~ Category +` |
| $f_{1, NP}(t_i) + f_{1, N}(t_i) +$  | `s(time, by = Category) +` |
| $f_{2, NP}(r_i) + f_{2, N}(r_i) +$  | `s(trial, by = Category) +` |
| $f_{3, NP}(t_i, r_i) + f_{3, N}(t_i, r_i) + \epsilon_i$  | `ti(time, trial, by = Category)`|


```{r}
m3.ti <- bam(y ~ Category + s(time, by = Category) + s(trial, by = Category) + ti(time, trial, by = Category), data = curves)
summary(m3.ti)
```

```{r echo=FALSE, fig.show="hold", out.width="50%", warning=FALSE}
plot_smooth(m3.ti, view = "time", plot_all = "Category", col = Category.colors, print.summary = FALSE)
plot_smooth(m3.ti, view = "trial", plot_all = "Category", col = Category.colors, print.summary = FALSE)
pvisgam(m3.ti, view = c("time", "trial"), select = 5, color = c('white', 'blue'), main = "ti(time,trial):CategoryNO_PEAK", print.summary = FALSE)
pvisgam(m3.ti, view = c("time", "trial"), select = 6, color = c('white', 'blue'), main = "ti(time,trial):CategoryPEAK", print.summary = FALSE)

```

In this case the non-linear interaction between `time` and `trial` appears to be significant (as there is little noise in the data), though its effect is minimal, judging from the contour levels. 

Another simplification of the model apparent from eye inspections is be to make the smooth over `trial` a linear term and independent of `Category`, i.e. substitute `s(trial, by = Category)` with just `trial`, or in maths notation $\beta_2 \cdot r_i$. The fact that the term is linear is suggsted by its shape and by the `edf` values of the respective smooths.



## Random effects

Just like we moved from `lm()` to `lmer()`, we can build mixed-effects GAMs, or GAMMs.
Random effects have the same meaning with GAMMs as with LMERs. The difference is that we can model variations in shape, i.e. **random smooths**. 

The `mgcv` library allows for three different sorts of random terms, which in practice allow for three degrees of complexity:

1. Random intercepts: only a constant
2. Random slopes: a constant plus a slope (tilt)
3. Random smooths: arbitrary shape (within the limit set by the number of splines `k`)

This is best illustrated in Fig. 6 at p. 10 of this [paper by van Rij et al.](https://journals.sagepub.com/doi/pdf/10.1177/2331216519832483).

Suppose we have curves along the time axis, the fixed factor `Category` as before, and subject (`subjId`) as random factor.
With `mgcv`, random intercepts are indicated as `s(subjId, bs = "re")`. This means that subject-specific effects only vary the global height of the `Category`-specific curves, i.e. they can shift those curves up or down but do not change their shape. This is the equivalent of `(1 | subjId)` in `lmer` notation.

Random slopes... it depends. These are a source of great confusion. If the slope is with respect to a covariate, say time, then slope means slope in the common sense, i.e. a tilted line. But just like with `lmer`, random slopes can be also specified with respect to a fixed factor, say `Category`, which will behave in the same manneer as with `lmer`, i.e. the 'slope' is the difference between the intercept and the specific level, e.g. `PEAK - NO_PEAK`. 

A random slope with respect to time would be: `s(subjId, t, bs = "re")`, while a random slope with respect to a fixed factor like `Category` would be: `s(subjId, Category, bs = "re")`. The first one is the one represented in the figure from the paper by van Rij. 
A random slope is equivalent to `(0 + t | subjId)`  in `lmer` notation. Note the explicit absence of intercept. This is because there is no way to estimate correlations between intercept and slope, as in `lmer`. If we want to model both random intercept and slope, we need to include both terms in the model, i.e. `s(subjId, bs = "re") + s(subjId, t, bs = "re")`, and yet the GAMM will not estimate the correlation between the two.

Finally, a random smooth is indicated as: `s(t, subjId, bs = "fs")`. Note that the first term indicates the covariate, the second the random factor. 

Each of the three setting can be further complexified by indicating a `by` factor, which behaves according to the conventions used in `mgcv`, i.e. for each level of the `by` factor an independent term is added. For example, a random intercept can be split by `Category` with: `s(subjId, by = Category, bs = "re")`, which adds two random intercepts, one for `PEAK`, one for  `NO_PEAK`. This achieves the same effect as `s(subjId, Category, bs = "re")`.

A random slope could be: `s(subjId, t, by = Category, bs = "re")`, where the slope is with respect to `t`, while `Category` simply splits the term in two as before. 

In the example below we use a random smooth split with respect to `Category`: `s(t, subjId, by = Category, bs = "fs")`.


### A case with subject-specific shape variations
A variation of the previous data set is depicted as follows:

```{r echo=FALSE, cache=TRUE, message=FALSE, warning=FALSE}
# Prior means
M <- c(m1 = 0.7, # first large peak
       s1 = 0.2,
       h1 = 3,
       m2 = 1.5, # second small peak
       s2 = 0.1,
       h2 = 1)
# Prior vars
S <- c(m1 = 0.1, # first large peak
       s1 = 0.02,
       h1 = 0.2,
       m2 = 0.05, # second small peak
       s2 = 0.02,
       h2 = 0.1)

nSubj <- 40
nTrials <- 20
nCurves <- nSubj * nTrials
# Construct a random cov matrix with the st. dev. above:
set.seed(90)
R <- rcorrmatrix(length(M))
C <- diag(S) %*% R %*% diag(S)
# generate subject-specific params
params <- rmvnorm(nSubj, M, C) %>%
  `colnames<-`(names(M)) %>%
  as_tibble() %>%
  mutate(subjId = factor(1:nSubj))
# generate curves according to params
s_e <- 0.3 # st. dev. of residual additive noise
t <- seq(0,2,by=0.1) # time axis interval
curves <- params %>%
  expand_grid(trialId = factor(1:nTrials), Category = c("PEAK", "NO_PEAK")) %>%
  mutate(curveId = factor(1:n())) %>%
  group_by(across(everything())) %>%
  summarise(t = t,
            y = case_when(
              Category == "PEAK"    ~ h1 * exp(-((t-m1)/s1)**2) + h2 * exp(-((t-m2)/s2)**2),
              Category == "NO_PEAK" ~ h1 * exp(-((t-m1)/s1)**2)
            )) %>%
  mutate(y = y + rnorm(n(), 0, s_e)) %>% # add residual noise
  ungroup() %>%
  select(-all_of(names(M))) %>%
  arrange(curveId, t) %>%
  mutate(across(c(subjId, Category, trialId, curveId), ~ factor(.x)))
# plot
curves %>%
  filter(subjId %in% 1:5 & trialId %in% 1:4) %>%
  ggplot() +
  aes(t, y, group = curveId, color = subjId) +
  facet_grid(~ Category, labeller = label_both) +
  geom_line() +
  scale_color_manual(values = colorblind_pal()(5)) +
  theme_light() +
  theme(text = element_text(size = 15))

```

The data set contains several `y` curves for each subject (a subset of subjects and trials in the plot above). Note that:

* All subjects globally behave similarly with respect to `Category`
* Each subject has their own departure from the general trend 
* The variation expressed on the first larger peak is consistent across `Category` levels within the same subject
* In addition, there is also a consistent, subject-specific departure from the mean in the second peak in `PEAK` condition, though not obviously related to the one of the first peak

We model all this by introducing a random smooth terms. Such terms vary along time, are speaker-dependent, i.e. are added to the fixed effects for each speaker, and come in two versions, one per `Category` level.

The model in `mgcv` is expressed as follows (we are not modelling the effect of trial):

```{r cache=TRUE, warning=FALSE}
m4 <- bam(y ~ Category + s(t, by = Category) +
            s(t, subjId, by = Category, bs = "fs", m = 1), data = curves)
summary(m4)
```
The `m = 1` argument is usually added to random smooths. The reason is to increase the non-linearity penalty, i.e. a stronger regularisation.

Let's compere it with a version without the random terms.

```{r cache=TRUE, warning=FALSE}
m4.fixed <- bam(y ~ Category + s(t, by = Category), data = curves)
summary(m4.fixed)
```

Note how R-sq and deviance explained are way higher for the model with random terms. In this case we can also directly affirm that the random terms are justified, as these are significant and they are the only difference between the two models. 

The summed fixed effects look as expected:

```{r}
plot_smooth(m4, view = "t", plot_all = "Category", col = Category.colors, print.summary = FALSE)
```

It is interesting to compare the result with the model without random terms:

```{r}
plot_smooth(m4.fixed, view = "t", plot_all = "Category", col = Category.colors, print.summary = FALSE)
```

Note how 'overconfident' the simpler model is!!



Let's visualise the shape of the random terms, picking out the first 5 subjects, to be compared with the raw data above:

```{r fig.show="hold", out.width="50%", warning=FALSE}
inspect_random(m4, select = 3, cond=list(subjId = 1:5 %>% factor(), Category = "NO_PEAK"), main = "NO_PEAK", col = colorblind_pal()(5), lty = 1, lwd = 2, print.summary = FALSE)
inspect_random(m4, select = 4, cond=list(subjId = 1:5 %>% factor(), Category = "PEAK"), main = "PEAK", col = colorblind_pal()(5), lty = 1, lwd = 2, print.summary = FALSE)
```


**Question:** Do you see find by eye inspection the subject-specific consistency in the change of shape of the first peak across `Category` levels in the plot above (comparing with the raw data further above)?

**Question:** Does the GAMM model such consistency explicitly?


### Practical advice

* Convert all character variables to factor
* Convert binary variables to integer
* GAMMs with random smooth slopes are **very very slow**, use a server, not your
  laptop, and use the nthreads option of bam()

  
  

### Resources

* Tutorial papers: [Martijn Wieling](http://martijnwieling.nl/files/GAM-tutorial-Wieling.pdf), [Marton Soskuthy](http://eprints.whiterose.ac.uk/113858/2/1703_05339v1.pdf)
* Online tutorials: [Joseph Roy](https://jroy042.github.io/nonlinear/), [Jacolien van Rij](http://jacolienvanrij.com/Tutorials/GAMM.html), [Peter Laurinec](https://petolau.github.io/Analyzing-double-seasonal-time-series-with-GAM-in-R/), [Noam Ross](https://noamross.github.io/gams-in-r-course/), [Michael Clark](https://m-clark.github.io/generalized-additive-models/)
