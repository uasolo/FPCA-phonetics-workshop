---
title: "GAMMs and time alignment"
author: "Michele Gubian"
date: "May 2023"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
library(tidyverse)
library(ggthemes)
library(magrittr)
library(mgcv)
library(itsadug)
library(emmeans)
library(RColorBrewer)
```

Here we present two datasets where curves do not have the same duration.

## Dataset 1

```{r }

# home_dir <- "/vdata/ERC2/FPCA/FPCA-phonetics-workshop"
data_dir <- '../data'
ex <- 1 
lin_t1 <- 0.65 # linear time distortion
curves <- read_csv(file.path(data_dir, paste("ex1D", ex, "csv", sep = '.'))) %>% 
  mutate(Category = factor(Category)) %>% 
  group_by(curveId) %>% 
  mutate(t1 = runif(1, lin_t1, 1/lin_t1) * time)
           
nCurves <- curves %>% select(curveId) %>% n_distinct()

Category.colors <- c("slategray4", "orangered")
Category.labels <- c(NO_PEAK = "NP (No Peak)", PEAK = "P (Peak)")
# plot a few curves
ggplot(curves %>%
         filter(curveId %in% sample(nCurves, 20)) %>%
         mutate(curveId = factor(curveId))) +
  aes(x = t1, y = y, group = curveId, color = curveId) +
  geom_line(linewidth = 0.3) +
  facet_grid(~ Category, labeller = labeller(Category = Category.labels)) +
  xlab("time") +
  theme_light() +
  theme(text = element_text(size = 15),
        legend.position = "none")

```

Let us investigate two common approaches to time normalisation prior to GAMM modelling. 

### Approach 1: No time normalisation

```{r fig.show="hold"}
m1 <- bam(y ~ Category + s(t1, by = Category), data = curves)
summary(m1)
plot_smooth(m1, view = "t1",  plot_all = "Category", 
            col = Category.colors,  print.summary = FALSE, rug = FALSE)
```

Notice that:

* The model does not complain if the curves have different durations
  - actually, it does not know there are curves!
* Confidence bands get wider towards the end
* Predicted curves are not capturing the salient second peak quite well (var explained 77%)

### Approach 2: Linear time normalisation

```{r fig.show="hold"}
ggplot(curves %>%
         group_by(curveId) %>% 
         mutate(t_lin = t1/max(t1)) %>% 
         ungroup() %>% 
         filter(curveId %in% sample(nCurves, 20)) %>%
         mutate(curveId = factor(curveId))) +
  aes(x = t_lin, y = y, group = curveId, color = curveId) +
  geom_line(linewidth = 0.3) +
  facet_grid(~ Category, labeller = labeller(Category = Category.labels)) +
  theme_light() +
  xlab("lin. norm. time") +
  theme(text = element_text(size = 15),
        legend.position = "none")
m2 <- bam(y ~ Category + s(t_lin, by = Category),
          data = curves %>%
            group_by(curveId) %>% 
            mutate(t_lin = t1/max(t1)))
summary(m2)
plot_smooth(m2, view = "t_lin",  plot_all = "Category", 
            col = Category.colors,  print.summary = FALSE, rug = FALSE)
```

In this case the result is excellent (var explained 97%). In fact, the data were generated by randomly applying a random linear time distorsion to each curve.

## Dataset 2

Suppose the data look like this instead:


```{r }
max_chop <- 0.6 
set.seed(28)
# plot a few curves
ggplot(curves %>%
         group_by(curveId) %>% 
         filter(time < runif(1, max_chop * max(time), max(time))) %>% 
         ungroup() %>% 
         filter(curveId %in% sample(nCurves, 10)) %>%
         mutate(curveId = factor(curveId))) +
  aes(x = time, y = y, group = curveId, color = curveId) +
  geom_line(linewidth = 0.3) +
  facet_grid(~ Category, labeller = labeller(Category = Category.labels)) +
  theme_light() +
  xlab("time") +
  theme(text = element_text(size = 15),
        legend.position = "none")

```

Let us apply the same approaches as in dataset 1.

### Approach 1: No time normalisation

```{r fig.show="hold"}
set.seed(28)
chop_curves <- curves %>% 
             group_by(curveId) %>% 
             filter(time < runif(1, max_chop * max(time), max(time))) %>% 
            ungroup()
m3 <- bam(y ~ Category + s(time, by = Category),
          data = chop_curves)
summary(m3)
plot_smooth(m3, view = "time",  plot_all = "Category", 
            col = Category.colors,  print.summary = FALSE, rug = FALSE)
```

This is the perfect solution in this case. In fact, the data were generated by chopping the curves randmoly towards the end.

### Approach 2: Linear time normalisation

```{r fig.show="hold"}
# plot a few curves
ggplot(chop_curves %>%
            group_by(curveId) %>% 
            mutate(t_lin = time/max(time)) %>%
         ungroup() %>% 
         filter(curveId %in% sample(nCurves, 10)) %>%
         mutate(curveId = factor(curveId))) +
  aes(x = t_lin, y = y, group = curveId, color = curveId) +
  geom_line(linewidth = 0.3) +
  facet_grid(~ Category, labeller = labeller(Category = Category.labels)) +
  xlab("linear norm. time") +
  theme_light() +
  theme(text = element_text(size = 15),
        legend.position = "none")
m4 <- bam(y ~ Category + s(t_lin, by = Category),
          data = chop_curves %>%
            group_by(curveId) %>% 
            mutate(t_lin = time/max(time)))
summary(m4)
plot_smooth(m4, view = "t_lin",  plot_all = "Category", 
            col = Category.colors,  print.summary = FALSE, rug = FALSE)
```

This time linear time normalisation creates distortions and artifacts in the solution.

## Interim conclusions

* GAMMs are agnostic and transparent to what we do to the time axis
* Time alignment has to be solved independently and prior to GAMM estimation
* There is no one-size-fits-all solution

# Correlation between shape and duration

## No categories

```{r}
lin_expansion <- 1.3
set.seed(28)
curves %>% 
  filter(Category  == "PEAK") %>% 
  group_by(curveId) %>% 
  mutate(across(c(y, time), ~ runif(1, 1/lin_expansion, lin_expansion) * .x)) %>%
  ungroup() %>% 
  inner_join(curves %>%
               ungroup() %>% 
               filter(Category  == "PEAK") %>%
               distinct(curveId) %>% 
               slice_sample(n = 8)) %>% 
  mutate(curveId = factor(curveId)) %>% 
  ggplot() +
  aes(time, y, group = curveId, color = curveId) +
  geom_line(linewidth=0.3) +
  xlab("time") +
  theme_light() +
  theme(text = element_text(size = 15),
        legend.position = "none")


```

### No time normalisation 
```{r, fig.show="hold", out.width="50%"}
set.seed(28)
m5 <- bam(y ~ s(time),
          data = curves %>% 
            filter(Category  == "PEAK") %>% 
            group_by(curveId) %>% 
            mutate(across(c(y, time), ~ runif(1, 1/lin_expansion, lin_expansion) * .x)))
summary(m5)
plot_smooth(m5, view = "time",  print.summary = FALSE, rug = FALSE)
```

### Linear time normalisation

```{r, fig.show="hold", out.width="50%"}
set.seed(28)
m6 <- bam(y ~ s(t_lin),
          data = curves %>% 
            filter(Category  == "PEAK") %>% 
            group_by(curveId) %>% 
            mutate(across(c(y, time), ~ runif(1, 1/lin_expansion, lin_expansion) * .x)) %>% 
            mutate(t_lin = time/max(time))
          )
summary(m6)
plot_smooth(m6, view = "t_lin",  print.summary = FALSE, rug = FALSE)
```

### Duration as covariate

```{r, fig.show="hold", out.width="50%"}
set.seed(28)
m7 <- bam(y ~ te(t_lin, duration, k = 10),
          data = curves %>% 
            filter(Category  == "PEAK") %>% 
            group_by(curveId) %>% 
            mutate(across(c(y, time), ~ runif(1, 1/lin_expansion, lin_expansion) * .x)) %>% 
            mutate(t_lin = time/max(time),
                   duration = max(time))
          )
summary(m7)
fvisgam(m7, view = c("t_lin", "duration"), color = c('white', 'blue'), main = "", print.summary = FALSE)
abline(h=c(2 / lin_expansion, 2 * lin_expansion),lty=2,lwd=2,col="red")
fvisgam(m7, view = c("t_lin", "duration"), plot.type = "persp", theta = 30, phi = 30, color = c('blue', 'white'), main = "", print.summary = FALSE, zlab = "y")
plot_smooth(m7, view = "t_lin", cond = list(duration = 2 / lin_expansion),  print.summary = FALSE, rug = FALSE, main = str_glue("Duration = {round(2 / lin_expansion, 2)}"), ylim = c(-0.1, 0.5))
plot_smooth(m7, view = "t_lin", cond = list(duration = 2 * lin_expansion),  print.summary = FALSE, rug = FALSE, main = str_glue("Duration = {round(2 * lin_expansion, 2)}"), ylim = c(-0.1, 0.5))
```

Notice that:

* The model captures somehow the correlation between duration and shape change
  - Though some parts of the time-by-duration space are not well modelled

## With categories

```{r}

lin_expansion_category <- function(curves, log_expansion = 0.25, sd_expansion = 0.1) {
  curves %>%
    group_by(curveId, Category) %>% 
    mutate(across(c(time, y), ~ case_when(
      Category == "PEAK" ~ .x * rnorm(1, log_expansion, sd_expansion) %>% exp(),
      Category == "NO_PEAK" ~ .x / rnorm(1, log_expansion, sd_expansion) %>% exp()
    ))) %>% 
    ungroup() %>% 
    mutate(curveId = factor(curveId))
}
set.seed(28)
ggplot(curves %>%
         lin_expansion_category() %>% 
         filter(curveId %in% sample(nCurves, 16))
         ) +
  aes(x = time, y = y, group = curveId, color = curveId) +
  geom_line(linewidth = 0.3) +
  facet_grid(~ Category, labeller = labeller(Category = Category.labels)) +
  xlab("time") +
  theme_light() +
  theme(text = element_text(size = 15),
        legend.position = "none")
```

### No time normalisation
```{r fig.show="hold"}
set.seed(28)
m8 <- bam(y ~ Category + s(time, by = Category),
          data = curves %>% lin_expansion_category())
summary(m8)
plot_smooth(m8, view = "time",  plot_all = "Category", 
            col = Category.colors,  print.summary = FALSE, rug = FALSE)
```

### Linear time normalisation
```{r fig.show="hold"}
set.seed(28)
ggplot(curves %>% 
         lin_expansion_category() %>% 
            group_by(curveId) %>% 
            mutate(t_lin = time / max(time))) +
  aes(x = t_lin, y = y, group = curveId, color = curveId) +
  geom_line(linewidth = 0.3) +
  facet_grid(~ Category, labeller = labeller(Category = Category.labels)) +
  theme_light() +
  xlab("lin. norm. time") +
  theme(text = element_text(size = 15),
        legend.position = "none")

set.seed(28)
m9 <- bam(y ~ Category + s(t_lin, by = Category),
          data = curves %>% lin_expansion_category() %>% 
            group_by(curveId) %>% 
            mutate(t_lin = time / max(time)))
summary(m9)
plot_smooth(m9, view = "t_lin",  plot_all = "Category", 
            col = Category.colors,  print.summary = FALSE, rug = FALSE)
```

### Duration as covariate
```{r, fig.show="hold", out.width="50%", cache=TRUE}
set.seed(28)
m10 <- bam(y ~ Category + te(t_lin, duration, by = Category, k = 10),
          data = curves %>% lin_expansion_category() %>% 
            group_by(curveId) %>% 
            mutate(t_lin = time/max(time),
                   duration = max(time))
          )
summary(m10)
fvisgam(m10, view = c("t_lin", "duration"), cond = list(Category = "NO_PEAK"),  color = c('white', 'blue'), main = "No Peak", print.summary = FALSE)
abline(h=c(1.5, 2.5),lty=2,lwd=2,col="red")
fvisgam(m10, view = c("t_lin", "duration"), cond = list(Category = "PEAK"),  color = c('white', 'blue'), main = "Peak", print.summary = FALSE)
abline(h=c(1.5, 2.5),lty=2,lwd=2,col="red")
fvisgam(m10, view = c("t_lin", "duration"), cond = list(Category = "NO_PEAK"), plot.type = "persp", theta = 30, phi = 30, color = c('blue', 'white'), main = "No Peak", print.summary = FALSE, zlab = "y")
fvisgam(m10, view = c("t_lin", "duration"), cond = list(Category = "PEAK"), plot.type = "persp", theta = 30, phi = 30, color = c('blue', 'white'), main = "Peak", print.summary = FALSE, zlab = "y")
plot_smooth(m10, view = "t_lin", plot_all = "Category", cond = list(duration = 1.5), col = Category.colors, lwd=2, main = "duration = 1.5", print.summary = FALSE, ylim = c(-0.1, 1.3))
plot_smooth(m10, view = "t_lin", plot_all = "Category", cond = list(duration = 2.5), col = Category.colors, lwd=2, main = "duration = 2.5", print.summary = FALSE, ylim = c(-0.1, 1.3))

```

Notice that:

* The two categories are estimated reliably only within their duration range
* Out of their duration range, estimates are off and with large confidence bands

This is to be expected because there are no samples of long NO_PEAK curves and of shot PEAK curves. 

Besides that, using duration as covariate in a categorical setting is conceptually wrong, as duration is part of how the curve looks, just like its shape, and the purpose of the model should be to predict how the curve looks given its category. Duration is not part of the experimental design, so it should be a response variable, not a predictor. 

## Alternative approaches to joint modelling of shape and duration

An approach based on multidimensional functional PCA allows to jointly analyse curves together with their time distortion representation. This is more general than applying linear time normalisation and it is based on *landmark registration*. 

See [this paper](https://ieeexplore.ieee.org/abstract/document/5947472), appendix A in [this paper](https://www.sciencedirect.com/science/article/abs/pii/S0167639317302017), [this paper](https://www.researchgate.net/profile/Michele-Gubian/publication/335829668_Zooming_in_on_Spatiotemporal_V-to-C_Coarticulation_with_Functional_PCA/links/5dff5f5ca6fdcc2837359757/Zooming-in-on-Spatiotemporal-V-to-C-Coarticulation-with-Functional-PCA.pdf) and material available in this repo (under `notes/` and `projects/`). 

