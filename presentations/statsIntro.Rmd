---
title: "A cursory view on linear models"
author: "Michele Gubian"
date: "9/10/2020"
output: html_document
# fig_width: 2
# fig_height: 1.5
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(magrittr)
library(lme4)
library(emmeans)
library(MASS)
select <- dplyr::select

```
# Ordinary linear models

## The simplest model

Let us (pretend to) have data from an EMA experiment. A speaker pronounces vowels.

* $x_i$ is the position of tongue dorsum in the $x$ direction of the sagittal plane

* $y_i$ is the corresponding F2

* $i$ is the running index



```{r echo = FALSE}
nSamp <- 50
x <- runif(nSamp, min = 30, max = 60)
y <-  900 + (x-30) * (1100/30) + rnorm(nSamp, 0, 50)
D <- tibble(x=x, y=y)
D %>% sample_n(10)

```

```{r}
ggplot(D) +
  aes(x, y) +
  geom_point()
```

A linear model has the form:
$$
y_i = \beta_0 + \beta_1 \cdot x_i + \epsilon_i
$$

* The data are the $x_i$ and the $y_i$
* The linear relationship linking the data is $y_i = \beta_0 + \beta_1 x_i$
* $\epsilon_i$ allows for unknown effects, preserving linearity
  + this is a strong hypothesis on the way  **unknown** or **not understood** interact with the known ones $x$ and $y$
* **Estimation** means to find suitable, credible values for $\beta_0$ and $\beta_1$
* **Interpreting** the model means to consider the role of $\beta_0$ and $\beta_1$
  + e.g. for a unit increase in $x$ we have a variation of $\beta_1$ in $y$ (in the appropriate units)

## What about Gaussianity? 

* What is Gaussian in a linear model?
* Does linearity imply Gaussianity?

```{r}
ggplot(D) +
  aes(x) +
  geom_density()

```

```{r}
ggplot(D) +
  aes(y) +
  geom_density()

```

* What is Gaussian is $\epsilon$
* $\epsilon_i$ are $N(0, \sigma^2)$ and *independent from each other* and have the same distribution (i.i.d.)
  - this is usually unreasonable, yet of great utitlity in simplifying calculations
  - keep an eye on independence, as this will be violated by GAMMs
* As a consequence, $y_i \sim N(\beta_0 + \beta_1 x_i, \sigma^2)$
* and the estimates of $\beta_0$ and $\beta_1$ are Gaussian too
  - This allows to build confidence intervals for them

* Does linearity imply Gaussianity?
  - No
  - The estimation of $\beta_0$ and $\beta_1$ does not require Gaussianity
  - We like Gaussianity because it allows to build confidence intervals for $\beta_0$ and $\beta_1$

## Linear models in R
  
So let's do this in R:

```{r}
lm1 <- lm(y ~ x, data = D)
summary(lm1)
```

* The *real* formula is $y_i = \beta_0 + \beta_1 x_i + \epsilon_i$
* y ~ x is the so-called "R formula" notation, NOT an equation
* Residuals are the estimated $\epsilon_i$
* (Intercept) is $\beta_0$, x is (confusingly) $\beta_1$
* p-values are there thanks to the Gaussianity hypothesis on $\epsilon_i$
* R-squared measures how much of the variance of  $y_i$ is explained by $y_i = \beta_0 + \beta_1 x_i$
  - will come back to this with LMER

The linear model looks like this:
```{r echo = FALSE}
ggplot(D) +
  aes(x, y) +
  geom_point() +
  geom_smooth(method='lm', formula= y~x, color = 'red')
```
... and it looks like a line,  hence the term "linear". In fact:

* $\beta_0$ is the intercept of the line on the $y$ axis
* $\beta_1$ is the line slope

However:

* The terms intercept and slope are used also in settings where they are more confusing than helpful
  - e.g. with factors (see below)

Check if the assumptions on $\epsilon$ are sensible:
```{r}
op <- par(mfrow=c(2,2))
plot(lm1)
par(op)
```

## Causality

Remember that a linear model detects correlation, not causation. 

E.g. take the previous data and model it `the wrong way around'
```{r}
lm1inv <- lm(x ~ y, data = D)
lm1inv %>% summary
```
## Non-linear relationships

What if the relation between $x$ and $y$ is non-linear? Can we still use a linear model?

```{r echo = FALSE}
nSamp <- 50
x <- runif(nSamp, min = -3, max = 3)
y <-  4 + x + 3 * x^3 + rnorm(nSamp, 0, 0.5)
D <- tibble(x=x, y=y)
ggplot(D) +
  aes(x, y) +
  geom_point()
```

Yes, e.g.:

$$
y_i = \beta_0 + \beta_1 \cdot x_i + \beta_2 \cdot x_i^2 + \beta_3 \cdot x_i^3 + \epsilon_i
$$
is linear in the $\beta$'s, i.e. the $\beta$'s only appear in sums or in multiplications with known quantities (the $x_i$'s)
```{r}
lm2 <- lm(y ~ x + I(x^2) + I(x^3), data = D)
summary(lm2)
```

Note that this is 'kind of' a GAM. 

The basic idea of GAM is to be able to determine automatically how many terms like $\beta_2 x_i^2$ to include in the model, as opposed to guess or try.

## Model selection

The previous data set could have been modelled also by:
$$
y_i = \beta_0 + \beta_1 \cdot x_i +  \beta_3 \cdot x_i^3 + \epsilon_i
$$
How to decide in general what's the best model? 

* **Model selection** is basically finding the simplest model consistent with
a set of data
* There are automated procedures based on information criteria
```{r}
step(lm2)
```

* But it is not always advisable to blindly rely on those procedures
  + e.g. you may have good reasons to keep a predictor based on previous solid evidence
  + or end up with too many variables just because you have a lot of data, which would hinder interpretability

## Factors

Example: 

* $x_i$ is the vowel, e.g. /a/, /i/, /o/, /u/

* $y_i$ is the corresponding F2

Now $x$ is a categorical variable, or factor. How do we make it into a number?

```{r echo=FALSE}
D <- tibble(x = gl(4, 50, labels = c('a', 'i', 'o', 'u')))
D %<>% mutate(y = case_when(
  x == "a" ~ 1200,
  x == "i" ~ 2300,
  x == "o" ~ 930,
  x == "u" ~ 900) + rnorm(n(), 0, 100))
D %>% head()
```

```{r}
lm3 <- lm(y ~ x, data = D)
summary(lm3)
```

The real equation looks like:
$$
y_i = \beta_{/a/} + \beta_{/i/} \cdot I(x_i = /i/) +  \beta_{/o/} \cdot I(x_i = /o/) +  \beta_{/u/} \cdot I(x_i = /u/) + \epsilon_i
$$

* lm() created indicator (or dummy) variables, i.e. taking values 0 or 1 on a logical statement
* they are 3, i.e. one minus the number of categories (factor levels)
* The intercept models the fourth left-out level, here /a/
  - if we had 4 indicators + intercept the equation would not be identifiable
  - R picks the first level in alphabetic order as intercept
  
Typical questions you want to answer using a factorial experiment 

* What's the expected (mean) F2 frequency for /i/?
* Are the F2 values of /o/ and /u/ distinguishable? 

Can we answer these questions from the summary?

* What's the expected (mean) F2 frequency for /i/?
  - $\beta_{/a/} + \beta_{/i/}$, i.e. Estimate of (Intercept) + Estimate of xi 
* Are the F2 values of /o/ and /u/ distinguishable (significantly different)? 
  - we cannot tell from the summary
  - we can only answer this question on pairs /a/ vs. another vowel
  - for the other pairs we need to compute post tests, or use contrasts
* Fortunately, all this mess is taken care of by the emmeans package

```{r}
emmeans(lm3, pairwise ~ x)
```

## Interactions between factors

Let's add gender to the vowels example


```{r echo=FALSE, message=FALSE}
D <- expand_grid(x = c('a', 'i', 'o', 'u'), g = c('M', 'F'))
D %<>% mutate(y = case_when(
  x == "a" & g == 'M' ~ 1200,
  x == "i" & g == 'M' ~ 2300,
  x == "o" & g == 'M' ~ 930,
  x == "u" & g == 'M' ~ 900,
  x == "a" & g == 'F' ~ 1400,
  x == "i" & g == 'F' ~ 2950,
  x == "o" & g == 'F' ~ 970,
  x == "u" & g == 'F' ~ 950)) %>%
  group_by(x, g) %>%
  group_modify(~ {.x$y + rnorm(50, 0, 50) %>% enframe(name = NULL, value = "y")})

```



```{r}
D %>% sample_n(2)
lm4 <- lm(y ~ x + g, data = D)
summary(lm4)
```

```{r}
emmeans(lm4, pairwise ~ g | x)
```
* The gender factor adds the same quantity to any vowel
* Conversely, the /i/ factor adds the same quantity to male and female
* What if e.g. females' /i/ has an even higher value than the one obtained adding the respective /i/ and female effects?

```{r}
lm4.1 <- lm(y ~ x * g, data = D)
summary(lm4.1)
emmeans(lm4.1, pairwise ~ g | x)
```



# Linear Mixed Effects Models

Let's have many subjects pronounce a vowel many times with or witout a certain condition $x$
We measure F1 in the middle of each vowel.

* $x = 0$ condition absent, $x = 1$ condition present
* $y$ is F1


```{r echo = FALSE}
set.seed(40)
nSubjects <- 20
D <- expand_grid(x = 0:1 %>% as.logical(), z = seq_len(nSubjects) %>% as.factor())
y0 <- 500
y1 <- 15
z.sd <- 100
zMean <- rnorm(nSubjects, 0, z.sd)
y.sd <- 50
D %<>% mutate(y = y0 + x * y1 + zMean[z]) %>%
  group_by(x, z) %>%
  group_modify(~ {.x$y + rnorm(15, 0, y.sd) %>% enframe(name = NULL, value = "y")})
```

```{r}
ggplot(D) + aes(x, y, color = x) + geom_boxplot() + theme(legend.position="top")
lm(y ~ x, data = D) %>% summary
```

Actually the data look like this when taking subjects ($z$) into account:
```{r}
ggplot(D) + aes(z, y, color = x) + geom_boxplot() + theme(legend.position="top")
```
How can we take this into account in a linear model?
```{r eval = FALSE}
lm(y ~ x + z, data = D)
```
This means that we have one indicator variable per subject, $z_1, z_2$, etc. But each new experiment will have different subjects, so $z_1$ does not mean anything in general. 

In other words, knowing something about subjects 1 to 20 does not tell us anything about subject 21, as there are no constraints on them.

We want to **add structure to the noise**, i.e.

* there are two sources of noise
* one is the particular way subjects differ from one another
* another is the particular way each utterance is produced
* with all this, we want to preserve linearity

The maths goes like this:


$$
y_{i, j} = \beta_0 + \beta_1 \cdot x_{i, j} + u_i + \epsilon_{i, j} \\
u_i \sim N(0, \sigma_u) \\
\epsilon_{i, j} \sim N(0, \sigma) \\
u_i,  \epsilon_{i, j} \; \text{ indipendent of each other}
$$
where $i$ is the subject index, $j$ is the utterance index within $i$. 

**Note on terminology:** the factor `x` is called **fixed**, while the speaker factor `z` is called **random**. The term random refers to the fact that the levels of `z` are by design a random extraction from a much larger population of possible levels (speakers in this case). Random does not mean that the effect of this factor on the model will be random, erratic or anything like that. 

Setting a factor as 'random' in a LMER simply means that we are constraining it to behave in a certain way, namely that its additive effect follows a zero-centered Gaussian distribution. In other words, we are hypothesising that these levels all belong to a distribution. If we add one more level, e.g. one more speaker, we already expect that its effect will be within a certain distribution, that its behaviour is unlikely to be completely different from the other speakers. This is a general practice known as *regularisation*, a strategy to get robust estimations. In practice, one can, and sometimes has to use it also outside the typical setting that gave random effects their name. For example, when the number of levels is high, say 10 or 20, even though these are all the levels there are, e.g. all the letters of the alphabet, and no random extraction from a larger set was made. In that case, it will be close to impossible to obtain reliable estimates if we set this factor as fixed. The only viable option is to set it as random, which only means that we are applying some constraints on it. Unfortunately, this also means that it is not possible to apply `emmeans` to those levels, i.e. no post-hoc tests. 

In R with package lme4:
```{r}
lmer1 <- lmer(y ~ x + (1|z), data = D) 
summary(lmer1)
emmeans(lmer1, pairwise ~ x)
```


## More structure on noise: random slopes

We want add the following information to the random component of the model:

* different subjects have a global level that varies
  - i.e.  random intercept, we've got it already, that's $u_i$
* different subjects perform the contrast $x$ differently, i.e. some more, some less on average
  - that's called a random slope (which is not a helpful name in this case)
* possibly, random intercepts and slopes are correlated
  - e.g. the higher than average subjects are, the less prominently they produce the contrast $x$
  

```{r echo = FALSE}
set.seed(30)
nSubjects <- 20
D <- expand_grid(x = 0:1 %>% as.logical(), z = seq_len(nSubjects) %>% as.factor())
y0 <- 500
y1 <- 15
z.sd <- 100
zx.sd <- 15
zx.cor <- -0.8
zMean <- mvrnorm(nSubjects, c(0,0), matrix(c(z.sd^2, zx.cor *z.sd* zx.sd, zx.cor *z.sd* zx.sd, zx.sd^2 ), nrow = 2)) %>%
  as.data.frame
D <- cbind(D, rbind(zMean, zMean)) %>% as_tibble()

y.sd <- 50
D %<>% mutate(y = y0 + x * y1 + V1 + V2 * x) %>%
  group_by(x, z) %>%
  group_modify(~ {.x$y + rnorm(15, 0, y.sd) %>% enframe(name = NULL, value = "y")})

ggplot(D) + aes(z, y, color = x) + geom_boxplot() + theme(legend.position="top")
```

The R formula notation to express the random effect structure is: `(x|z)`. It is a short for  `(1 + x|z)`, which says it incorporates both random intercept and slope. It is *not* the same as `(1 |z) + (0 + x|z)`, because the latter specifies a random elements structure where random intercept and slope are not correlated, while with `(1 + x|z)` we also get an estimate of their correlation.

```{r}
lmer1 <- lmer(y ~ x + (x|z), data = D) 
lmer1 %>% summary
emmeans(lmer1, pairwise ~ x)

```

The estimate of the correlation between random intercept and slope (row `xTRUE`, column `Corr` under `Random effects`) confirms that they are negatively correlated.

## Inspecting random effects

Just like you can look (plot) the residuals in a lm, you can look at the random intercepts and slopes:

```{r}
lmer1.ranef <- lmer1 %>% ranef() %>% .$z %>% as_tibble() %>% rownames_to_column("Subject")
ggplot(lmer1.ranef) +
  aes(`(Intercept)`, xTRUE, label = Subject) +
  geom_text() +
  xlab("Random intercept") +
  ylab("random slope")
```

## Proportion of explained variance ($R^2$)

There is no straightforward way to get $R^2$ like for lm(). 
An estimate has been proposed, **Pseudo-R-squared**, which allows to get info on proportion of variance explained by fixed and random factors.

* Marginal $R^2$: proportion of variance explained by fixed effects
* Conditional $R^2$: proportion of variance explained by fixed and random effects combined
  - i.e. all except $\epsilon$

```{r warning=FALSE}
library(MuMIn)
r.squaredGLMM(lmer1)
```

E.g. useful to have an idea of how large is the inter-subject variation compared to the general (fixed) effect under investigation.


# Multiresponse models

Suppose we want to predict the value of both F1 and F2 for each given vowel. 
Can we do this? 

Short answer:

* If there are no random effects: yes
* If there are random effects: more no than yes

## No random effects

Example:

* $x_i$ is the vowel, e.g. /a/, /i/, /o/, /u/

* $F1_i$ and $F2_i$ are the corresponding first two formats

```{r echo=FALSE}
D <- tibble(x = gl(4, 50, labels = c('a', 'i', 'o', 'u')))
D %<>% mutate(
  F1 = case_when(
  x == "a" ~ 830,
  x == "i" ~ 300,
  x == "o" ~ 450,
  x == "u" ~ 400) + rnorm(n(), 0, 100),
  F2 = case_when(
  x == "a" ~ 1200,
  x == "i" ~ 2300,
  x == "o" ~ 930,
  x == "u" ~ 900) + rnorm(n(), 0, 100)
  )
```

The equation looks like this:
$$
F1_i = \beta_{F1, /a/} + \beta_{F1, /i/} \cdot I(x_i = /i/) +  \beta_{F1, /o/} \cdot I(x_i = /o/) +  \beta_{F1, /u/} \cdot I(x_i = /u/) + \epsilon_{F1, i} \\
F2_i = \beta_{F2, /a/} + \beta_{F2, /i/} \cdot I(x_i = /i/) +  \beta_{F2, /o/} \cdot I(x_i = /o/) +  \beta_{F2, /u/} \cdot I(x_i = /u/) + \epsilon_{F2, i} 
$$
In practice, these are two almost independent models, as all the $\beta$ terms for the two formants are distinct and independent. What links the two models is the fact that the two $\epsilon$ terms are estimated jointly and their correlation is also estimated (i.e. the two $\epsilon$ belong to a two dimensional random variable with Gaussian distribution, zero mean and a general 2-by-2 covariance matrix).

In R, we can use `lm`:

```{r}
mlm1 <- lm(cbind(F1, F2) ~ x, data = D)
summary(mlm1)
emmeans(mlm1, pairwise ~ x | formant, mult.name = "formant")
```
More resources on this:

* [Appendix to "An R Companion to Applied Regression" by Fox & Weisberg](https://socialsciences.mcmaster.ca/jfox/Books/Companion/appendices/Appendix-Multivariate-Linear-Models.pdf)
* [Post by University of Virginia](https://data.library.virginia.edu/getting-started-with-multivariate-multiple-regression/)

## Multiresponse LMER

There are two directions I would try, one is frequentist, the other Bayesian. 

* Frequentist: R library `mcglm`. It is quite rich and you can do basically everything **except** post-hoc comparisons. It does not interface with `emmeans`, nor there is any indication that such tests will be available in the future (see [this post by Bonat](https://stats.stackexchange.com/questions/363921/pairwise-comparisons-on-mcglm-joint-model-in-r
))
* Bayesian: using R library `brms`, which is based on the *Stan* engine, it is possible to define and estimate multiresponse LMERs. But Bayesian methods are beyond the scope of these notes

