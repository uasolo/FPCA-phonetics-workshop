---
title: "A cursory view on Generalised Additive (Mixed) Models (GAMMs)"
author: "Michele Gubian"
date: "March 6, 2022"
output: html_document
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(magrittr)
library(mgcv)
library(itsadug)
library(emmeans)

```

# A naive approach to modelling curves

Suppose we have several realisations of a pitch accent in two conditions.
For simplicity:

* no random factors, e.g. all utterances from the same speaker
* all utterances examined on the same time interval, 0-2 sec
* or we might imagine that limear time normalisation was applied


```{r echo=FALSE, message=FALSE}
home_dir <- "/vdata/ERC2/FPCA/FPCA-phonetics-workshop"
data_dir <- file.path(home_dir,'data')
ex <- 1 
curves <- read_csv(file.path(data_dir, paste("ex1D", ex, "csv", sep = '.')))
nCurves <- curves %>% select(curveId) %>% n_distinct()
curves %<>% mutate(Category = factor(Category))

Category.colors <- c("slategray4", "orangered")
# plot a few curves
ggplot(curves %>% filter(curveId %in% sample(nCurves, 20))) +
  aes(x = time, y = y, group = curveId, color = Category) +
  geom_line() +
  scale_color_manual(values=Category.colors) +
  theme_light() +
  theme(text = element_text(size = 15))

```

We could try something like this and use `lm()`:

$$
y_i = \beta_0 + \beta_{0, P} + \beta_1 t_i + \beta_{1, P} t_i +   \beta_2 t_i^2 + \beta_{2, P} t_i^2 + 
\beta_3 t_i^3 + \beta_{3, P} t_i^3 + ... + \epsilon_i 
$$
where:

* $P$ indicates the PEAK category, while the default (control) level is NO_PEAK
* instead of $x$ we have $t$ for time
* $i$ is **not** an index for an entire curve, but for a single sample

In R it would look like this:

```{r echo=TRUE, eval=FALSE}
mod <- lm(y ~ Category + time + Category:time + I(time^2) + Category:I(time^2) + I(time^3) + Category:I(time^3) , data = curves)
```

although this is quite a naive approach, see orthogonal polynomials (`poly` in R).

### Problems

* We don't know where to stop with power terms
    + but we could use model selection 
* Pure power series are really bad at interpolating general shapes
    + because Taylor's theorem is about approximating curves near a specific point, not globally
* How are we going to use the $\beta$ terms for insight and interpretation?

# GAMMs

* Use **SPLINES** as base (polynomial) functions, because they are good for interpolating any shape
* Use **regularisation** and **Cross-Validation** to determine complexity (how many functions) and smoothness 
* **Interpret shapes**, not coefficients

[Slides by Simon Wood](https://www.maths.ed.ac.uk/~swood34/talks/Beijing15.pdf)

The model looks like:

$$
y_i = f(t_i) + f_P(t_i) + \epsilon_i 
$$
where each $f()$ is a SPLINE:
$$
f(t) = \sum_{k=1}^K \beta_k b_k(t)
$$
In the GAMMs lingo, the $f()$ are called **smooths**. 
In simple terms, we can imagina a smooth to be like a polynomial of unspecified degree, or in general a functional term of unspecified complexity, i.e. we do not say how many terms it should include. The GAMM machinary takes care of determining how many terms are strictly needed for our problem. 

## GAMMs in R

* Package `mgcv` is the most popular for GAMMs within R
* The main command is `bam()`, equivalent to `lmer()` in `lme4`
* The R `formula` syntax is quite different from the one used in `lme4`
* The model specification with `bam()` has more possibilities than `lmer()`
    + fixed and random predictors can be declaired as linear (like with `lmer()`) or as smooths, or combinations
    + interactions between covariates (= continuous predictors) has different options based on *tensors*
* Interaction between factors (`a * b`) **is not directly available**
    + the user has to create their own `interaction` terms manually beforehand (more on this later)


## Specifying fixed effects 

### A single smooth

| Equation  | R formula |
|:---------:|:---------:|
| $y_i = \beta_0 + f(t_i) + \epsilon_i$ | `y ~ s(time)` |

Note that regular smooths like `s(time)` are **centered on zero**, hence the intercept term in the equation. The model `summary()` will show two separate terms: 

* `(Intercept)`, i.e. $\beta_0$
* `s(time)`, i.e. the zero-centered function $f(t_i)$

Example:
```{r echo=FALSE, message=FALSE}
# plot a few curves
ex <- 2
curves <- read_csv(file.path(data_dir, paste("ex1D", ex, "csv", sep = '.'))) %>%
  filter(Category == "NO_PEAK") %>%
  mutate(y = y + rnorm(n(), 0, 0.2))
ggplot(curves %>% filter(curveId %in% sample(nCurves, 20)) %>% mutate(curveId = factor(curveId)) ) +
  aes(x = time, y = y, group = curveId, color = curveId) +
  geom_line() +
  theme_light() +
  theme(text = element_text(size = 15), legend.position = "none")
```



```{r}
m1 <- bam(y ~ s(time), data = curves)
summary(m1)
```
### Reading the summary

* `Parametric coefficients`: the `lm`-style fixed effects
* `edf`: estimated degrees of freedom of smooth terms. More or less, how many coefficients you need to specify the shape of a smooth. The default max number of coefficients is `k = 10`, minus 1, as the smooth is zero-centered, so the intercept is already specified
* `fREML`: fast restricted maximum likelihood estimation. Cannot be used to compare models with different fixed effects composition. For this you need to re-run your models in `ML` modality, see section 4.5.1 in  [Wieling's tutorial](http://martijnwieling.nl/files/GAM-tutorial-Wieling.pdf).
* `Scale est.`: residual variance
* `n`: number of samples
    + number of curves? missing! (more on this later)

### Plotting

* Plotting is essential!
* Make sure you know what you are plotting (see [itsadug  docs](https://cran.r-project.org/web/packages/itsadug/vignettes/overview.html)):
    + partial effects
    + summed effects
    
**Partial effects** are smooths, which are zero centered in their default mode. This means that they represent the 'shape part' of the effect, i.e. the total effect minus the intercept.

Smooths are (implicitly) numbered by they order of appearance in the `summary()` output. To plot a selected smooth, two options (among many):

First option: use `mgcv::plot`:
```{r}
plot(m1, select = 1, shade = TRUE, rug = FALSE)
abline(h=0, col='red', lty=2)
```

Second option: extract fitting data from model, then plot it with `ggplot` or `plotfunctions::plot_error`.
```{r}
smooth1 <- get_modelterm(m1, select = 1)
smooth1$time
smooth1$fit
smooth1$se.fit
ggplot(smooth1) +
  aes(x = time, y = fit) +
  geom_line() +
  geom_ribbon(mapping = aes(ymin = fit - se.fit, ymax = fit + se.fit), fill = "red", alpha = 0.5) +
  theme_light() +
  theme(text = element_text(size = 15))
```

**Summed effects** include smooths and intercept (parametric) terms that pertain to an effect. In our case it means $\beta_0 + f(t_i)$.

The easy way to plot summed effects is to use `itsadug::plot_smooth`:
```{r}
plot_smooth(m1, view = "time", rug = FALSE)
```
One could also get all the relevant values and sum them manually. In our case, get samples from the smooth $f(t_i)$ using `get_modelterm` as above, then add the value of $\beta_0$ taken form the summary (i.e. `m1$coefficients["(Intercept)"]`).

**Question:** the confidence intervals are quite thin in comparison with the impression of noisy signals from the raw data plot. Why is that?

### Diagnostics



#### Degrees of freedom

The specification of smooths `s()` has an extra argument `k` whose default value is 10. `k` indicates the maximum number of splines allowed to specify the smooth, or in other words the maximum number of parameters or degrees of freedom.
As regular smooths are zero-centered, this means that a smooth with at most `k = 10` parameters has actually at most `k' = 9` degrees of freedom, as one is absorbed by the intercept term. The estimated degrees of freedom `edf` is an indicator for how many 'equivalent parameters' were actually utilised in the estimated smooth. When `edf` is near 1, it means that it's basically a straight line (only the slope parameter). On the contrary, when `edf` is very close to `k' = k-1` as in this case, it means that it has used up all its degrees of freedom, thus perhaps more are needed. 

Let's specify a higher `k` and see the what happens to `edf` for `s(time)`:

```{r}
m1.k20 <- bam(y ~ s(time, k = 20), data = curves)
summary(m1.k20)
```
`edf` has not increased much, and it's well below `k - 1 = 19`, so the limit on `k = 10` was ok.

#### Residuals

```{r}
gam.check(m1)
```

* Quantile plot and histogram indicate how close to Gaussian the distribution of residuals is, i.e. how close we are to the hypothesis $\epsilon \sim N(0, \sigma)$.
* Residuals vs. linear predictor shows whether trends are visible in the residuals -- which they shouldn't. For example, if residuals have a wider span for some values of the predicted output it means that the data are heteroscedastic, i.e. the variance of the residuals is not independent of predicted output, but it varies with it. Sometimes, especially with time series, this problem is mitigated by using a different distribution for the residuals, the so-called *scaled-t*. But often it also means that the model is ill-specified, and a different predictor structure is needed.
* Response vs. For Gaussian residuals, Fitted Values provides a similar picture as Residuals vs. linear predictor. The difference is that the y axis reports the actual output values (`y` in the example), and not the residual errors.



### A fixed factor interacting with a smooth

```{r echo=FALSE, message=FALSE}
ex <- 1 
curves <- read_csv(file.path(data_dir, paste("ex1D", ex, "csv", sep = '.')))
nCurves <- curves %>% select(curveId) %>% n_distinct()
curves %<>% mutate(Category = factor(Category))

# plot a few curves
ggplot(curves %>% filter(curveId %in% sample(nCurves, 20))) +
  aes(x = time, y = y, group = curveId, color = Category) +
  geom_line() +
  theme_light() +
  scale_color_manual(values=Category.colors) +
  theme(text = element_text(size = 15))
```

| Equation  | R formula |
|:---------:|:---------:|
| $y_i = \beta_0 + \beta_P + f_{NP}(t_i) + f_P(t_i) + \epsilon_i$ | `y ~ Category + s(time, by = Category)` |

where $P$ indicates the PEAK category, while the default (control) level is NO_PEAK ($NP$), and $i$ is the sample (not curve) index.
The equation is omitting the indicator functions. The explicit version is:
$$ y_i = \beta_0 + \beta_P \cdot I(i \in P) + f_{NP}(t_i) \cdot I(i \in NP) + f_P(t_i) \cdot I(i \in P) + \epsilon_i $$
Note the asymmetry in the conventions for the parametric and the smooth parts of the model (!!):

* Parametric part: as in `lm()`, $\beta_0$ is the intercept, $\beta_P$ is the difference between level $P$ and the intercept
* Smooths: one for each level of the fixed factor, all zero-centered

**Question:** What are the summed effects for the PEAK and the NO_PEAK levels (in maths notation)?

Estimate the model with `bam()`:
```{r}
curves %<>% mutate(Category = factor(Category)) # don't forget!
m2 <- bam(y ~ Category + s(time, by = Category), data = curves)
summary(m2)
```
**Practical tip:** all variables that need to be used as fixed or random factors (here `Category`) **must** be converted to R class `factor`, otherwise `bam` will throw an error. 

**Question:** is PEAK significantly different from NO_PEAK?



We can read off the `summary` output the statistical significance of parametric terms in the same way as with `lm`. In geometrical terms, we can say whether the global level of `y` (regardless of time) is different for PEAK and NO_PEAK levels. 

With the model expressed as above, we **cannot** use the significance levels in the  `summary` to answer questions about the difference between smooths. A small p-value only says that the smooth is significantly different from zero, i.e. from the flat line `y = 0`. If both PEAK and NO_PEAK smooths appear with a small p-value, it means that their shapes are justified, i.e. in both cases `y` varies non-linearly with `time`. Nothing can be said about whether the way it varies for PEAK is different from the way it varies for NO_PEAK.

Let's start plotting first. A look at the summed effects:
```{r}
plot_smooth(m2, view = "time", plot_all = "Category", col = Category.colors)
```
A graphical way to detect statistical differences between effects is:
```{r}
plot_diff(m2, view = "time", comp = list(Category = c("PEAK", "NO_PEAK")))
```

### Testing for significance: `emmeans`

The powerful `emmeans` function can be used on `mgcv` GAMMs as a way to test pointwise comparisons. Suppose we want to test whether PEAK and NO_PEAK differ at `time` equal 0.0, 1.0 and 2.0. We can use `emmeans` in the same way as we do with `lm` or `lmer`:

```{r}
emmeans(m2, pairwise ~ Category | time, at = list(time = 0:2))
```

### Testing for significance: binary smooths

While `emmeans` allows to test differences at specific points on the time axis (or other continuous dimensions), we need a way to assess for the significance of a smooth contrast as a whole. For the purpose, we utilise another way to specify a smooth interacting with a fixed factor with `mgcv::bam`, the so-called *binary smooths*.

First we need to define **numeric variables with the meaning of a logical variables** (yes you read it right), which represent the desired contrasts -- a rather obscure convention adopted in the `mgcv` library. In our case we want to represent the difference between PEAK and  NO_PEAK. With NO_PEAK as control level:
```{r}
curves$IsPEAK <- (curves$Category == "PEAK") * 1
```
where  `* 1` is a quick way to transform the logical value inside `()` into a numeric one. The R formula and the corresponding maths form are:

| Equation  | R formula |
|:---------:|:---------:|
| $y_i = \beta_0 + f_0(t_i) + b_P(t_i) \cdot I(i \in P) + \epsilon_i$ | `y ~ s(time) + s(time, by = IsPEAK)` |

where:

* $\beta_0$ is the global intercept
* $f_0(t_i)$ is the global, zero-centered smooth
* $b_P(t_i)$ is **not zero-centered** binary smooth. It represents the total difference between PEAK and  NO_PEAK

```{r}
m2.bin <- bam(y ~ s(time) + s(time, by = IsPEAK), data = curves)
summary(m2.bin)
```
| Maths  | R summary |
|:---------:|:---------:|
| $\beta_0$ | `(Intercept)` |
| $f_0(t_i)$  | `s(time)` |
| $b_P(t_i)$ | `s(time):IsPEAK` |

Only now we are able to report about the significance of the difference between PEAK and NO_PEAK by reading off the `summary` output for `s(time):IsPEAK`.

How does the difference look?
```{r}
get_modelterm(m2.bin, select = 2) %>%
  ggplot() +
  aes(x = time, y = fit) +
  geom_line() +
  geom_ribbon(mapping = aes(ymin = fit - se.fit, ymax = fit + se.fit), fill = "red", alpha = 0.5) +
  theme_light() +
  theme(text = element_text(size = 15))
```

There is a third modality for smooths, the so-called *ordered factor difference smooth*. It allows to separate the intercept difference form the shape difference, i.e. it splits the $b_P(t_i)$ term into a constant and a zero-centered smooth. See section 4.5.3 in [Wieling's tutorial](http://martijnwieling.nl/files/GAM-tutorial-Wieling.pdf) for details. 

## Two interacting covariates -- smooth surfaces

Suppose we want to model the effect of trial, i.e. the shape of the curve `y(time)` does change not only according to a category (PEAK, NO_PEAK) but also along a numeric variable `trial` encoding at which point in the experiment we are. The data looks like this:

```{r echo=FALSE, message=FALSE}
ex <- 1 
curves <- read_csv(file.path(data_dir, paste("ex1D", ex, "csv", sep = '.')))
nCurves <- curves %>% select(curveId) %>% n_distinct()
h1_ <- -3e-3; m1_ <- 1; s1_ <- .5
curves %<>% 
  inner_join(curves %>% distinct(curveId) %>% mutate(trial = sample(n(), replace = TRUE))) %>%
  mutate(across(c(Category, curveId), ~ factor(.x))) %>%
  group_by(curveId) %>%
  mutate(y = y + trial * h1_ * exp(-((time-m1_)/s1_)**2)) %>%
  ungroup()

rm(h1_, m1_, h1_)  

# plot a few curves
ggplot(curves %>% filter(curveId %in% sample(nCurves, 20))) +
  aes(x = time, y = y, group = curveId, color = trial) +
  geom_line() +
  facet_grid(~ Category) +
  theme_light() +
  theme(text = element_text(size = 15))
```
This is a case of a two-dimensional, nonlinear interaction between two covariates, `time` ($t$) and `trial` $(r)$. The default way is to use a *tensor*. 

| Equation  | R formula |
|:---------:|:---------:|
| $y_i = \beta_0 + \beta_P + f_{NP}(t_i, r_i) + f_P(t_i, r_i) + \epsilon_i$ | `y ~ Category + te(time, trial, by = Category)` |

Each of the tensor functions $f$ is the generalisation of a complete interaction expression in `lm`:

| Maths      | R notation in lm() |
|:---------:|:----------:|
| $\beta_1 \cdot t_i + \beta_2 \cdot r_i + \beta_3 \cdot t_i \cdot r_i$ | `time * trial` |

```{r}
m3 <- bam(y ~ Category + te(time, trial, by = Category), data = curves)
summary(m3)
```
The surface plot for the PEAK and NO_PEAK conditions and slices along `trial == 20` and `trial == 80` (all summed effects) are:

```{r echo=FALSE, warning=FALSE, fig.show="hold", out.width="50%",message=FALSE}
fvisgam(m3, view = c("time", "trial"), cond = list(Category = "NO_PEAK"), color = c('white', 'blue'), main = "NO_PEAK", print.summary = FALSE)
abline(h=c(20, 80),lty=2,lwd=2,col="red")
fvisgam(m3, view = c("time", "trial"), cond = list(Category = "PEAK"), color = c('white', 'blue'), main = "PEAK", print.summary = FALSE)
abline(h=c(20, 80),lty=2,lwd=2,col="red")
plot_smooth(m3, view = "time", plot_all = "Category", cond = list(trial = 20), col = Category.colors, lwd=2, main = "trial = 20", print.summary = FALSE)
plot_smooth(m3, view = "time", plot_all = "Category", cond = list(trial = 80), col = Category.colors, lwd=2, main = "trial = 80", print.summary = FALSE)

```

If we were using `lm` instead of `bam` we would be able to produce similar surface visualisations, but in this case pretty bad estimates:

```{r}
m3.lm <- lm(y ~ Category * time * trial, data = curves)
 summary(m3.lm)
```
```{r echo=FALSE, warning=FALSE, fig.show="hold", message=FALSE}
# construct predictions on a regular grid
m3.lm.pred <- expand_grid(curves %>% distinct(Category, time), trial = 1:100) %>%
  mutate(y = predict(m3.lm, .))
ggplot(m3.lm.pred) +
  aes(time, trial, fill = y) +
  # stat_contour_filled(bins = 9) + 
  geom_raster() +
  facet_grid(~ Category) +
  # scale_fill_brewer() +
  scale_fill_gradient(low = 'white', high = 'blue') +
  geom_hline(yintercept = c(20, 80), linetype = "dashed", color = "red") +
  theme_light() +
  theme(text = element_text(size = 15),
        legend.position = "bottom")

m3.lm.pred %>%
  filter(trial %in% c(20, 80)) %>%
  ggplot() +
  aes(time, y, color = Category) +
  geom_line() +
  scale_color_manual(values=Category.colors) +
  facet_grid(~ trial, labeller = label_both) +
  theme_light() +
  theme(text = element_text(size = 15), legend.position = "bottom")
  
```

The surfaces themselves are non-linear, i.e. they are not flat, because of the product term $\beta_3 \cdot t_i \cdot r_i$, but each slice along either `time` or `trial` is a line (this is called a *ruled surface*). 

To test for significance differences between PEAK and NO_PEAK, we can use binary smooth surfaces as above:

```{r}
curves$IsPEAK <- (curves$Category == "PEAK") * 1
m3.bin <- bam(y ~ te(time, trial) + te(time, trial, by = IsPEAK), data = curves)
summary(m3.bin)
pvisgam(m3.bin, view = c("time", "trial"), select = 2, color = c('white', 'blue'), main = "PEAK - NO_PEAK", print.summary = FALSE)
```

The difference PEAK - NO_PEAK is then significant, but the surface plot suggests that such difference does not change much along `trial`. To disentangle this, another form of tensor product is available, `ti`, which isolates the interaction:

| Equation  | R formula |
|:---------|:---------|
| $y_i = \beta_0 + \beta_P +$ | `y ~ Category +` |
| $f_{1, NP}(t_i) + f_{1, N}(t_i) +$  | `s(time, by = Category) +` |
| $f_{2, NP}(r_i) + f_{2, N}(r_i) +$  | `s(trial, by = Category) +` |
| $f_{3, NP}(t_i, r_i) + f_{3, N}(t_i, r_i) + \epsilon_i$  | `ti(time, trial, by = Category)`|


```{r}
m3.ti <- bam(y ~ Category + s(time, by = Category) + s(trial, by = Category) + ti(time, trial, by = Category), data = curves)
summary(m3.ti)
```

```{r echo=FALSE, fig.show="hold", out.width="50%", warning=FALSE}
plot_smooth(m3.ti, view = "time", plot_all = "Category", col = Category.colors, print.summary = FALSE)
plot_smooth(m3.ti, view = "trial", plot_all = "Category", col = Category.colors, print.summary = FALSE)
pvisgam(m3.ti, view = c("time", "trial"), select = 5, color = c('white', 'blue'), main = "ti(time,trial):CategoryNO_PEAK", print.summary = FALSE)
pvisgam(m3.ti, view = c("time", "trial"), select = 6, color = c('white', 'blue'), main = "ti(time,trial):CategoryPEAK", print.summary = FALSE)

```

In this case the non-linear interaction between `time` and `trial` appears to be significant (as there is little noise in the data), though its effect is minimal, judging from the contour levels. 

Another simplification of the model apparent from eye inspections is be to make the smooth over `trial` a linear term and independent of `Category`, i.e. substitute `s(trial, by = Category)` with just `trial`, or in maths notation $\beta_2 \cdot r_i$. The fact that the term is linear is suggsted by its shape and by the `edf` values of the respective smooths.

### Random smooths

Just like we moved from lm() to lmer(), we can build mixed-effects GAMs, or GAMMs.

* Random effects become random smooths
* These can model 'intercept' and/or 'slopes', i.e.:
  - intercept: e.g. each speaker has a specific f0 shape that departs from the mean f0 contour for all speakers
  - slope: e.g. each speaker has a specific way of realising a contrast expressed in the entire f0 contour
* Smooth (functional) correlation between smooth random intercepts and slopes cannot be modelled with the mgcv package


### What is special about time-varying signals?

* What is the difference between time-series and other types of functional data?
* When we provide information (to the model) about the grouping of smooths induced by having different subjects, is there any other relationship in the data we would like to represent?

We would like to indicate which $(t_i, y_i)$ samples belong to the same curve! There are two ways of doing it:

* Define another (nested) level of random smooth, call it the token
  - but it is rarely done in practice, as it is likely to fail to converge (but see [this paper by van Rij et al.](https://journals.sagepub.com/doi/pdf/10.1177/2331216519832483))
* Alternatively, enrich $\epsilon$ with an AR1 model to express correlation of adjacent samples
  - $\epsilon_i = \rho \epsilon_{i-1} + \psi_i$

### Practical advice

* Convert all character variables to factor
* Convert binary variables to integer
* GAMMs with random smooth slopes are **very very slow**, use a server, not your
  laptop, and use the nthreads option of bam()

  
  

### Resources

* Tutorial papers: [Martijn Wieling](http://martijnwieling.nl/files/GAM-tutorial-Wieling.pdf), [Marton Soskuthy](http://eprints.whiterose.ac.uk/113858/2/1703_05339v1.pdf)
* Online tutorials: [Joseph Roy](https://jroy042.github.io/nonlinear/), [Jacolien van Rij](http://jacolienvanrij.com/Tutorials/GAMM.html), [Peter Laurinec](https://petolau.github.io/Analyzing-double-seasonal-time-series-with-GAM-in-R/), [Noam Ross](https://noamross.github.io/gams-in-r-course/), [Michael Clark](https://m-clark.github.io/generalized-additive-models/)
